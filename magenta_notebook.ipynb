{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magenta.models.rl_tuner import rl_tuner\n",
    "from magenta.models.rl_tuner import rl_tuner_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定输出文件存储路径\n",
    "SAVE_PATH = \"/tmp/rl_tuner/2020.3.7-03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter settings\n",
    "ALGORITHM = 'q'\n",
    "REWARD_SCALER = 1\n",
    "OUTPUT_EVERY_NTH = 50000\n",
    "NUM_NOTES_IN_COMPOSITION = 64\n",
    "PRIME_WITH_MIDI = False\n",
    "#NOTE_RNN_CHECKPOINT_DIR = '/root/magenta/magenta/models/rl_tuner/checkpoint'\n",
    "#NOTE_RNN_CHECKPOINT_FILE = '/root/magenta/magenta/models/rl_tuner/checkpoint/note_rnn.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_tuner_hparams = tf.contrib.training.HParams(random_action_probability=0.1,\n",
    "                                               store_every_nth=1,\n",
    "                                               train_every_nth=5,\n",
    "                                               minibatch_size=32,\n",
    "                                               discount_rate=0.5,\n",
    "                                               max_experience=100000,\n",
    "                                               target_network_update_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_net = rl_tuner.RLTuner(SAVE_PATH, \n",
    "                          dqn_hparams=rl_tuner_hparams, \n",
    "                          algorithm=ALGORITHM,\n",
    "                          reward_scaler=REWARD_SCALER,\n",
    "                          output_every_nth=OUTPUT_EVERY_NTH,\n",
    "                          num_notes_in_melody=NUM_NOTES_IN_COMPOSITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate initial music sequence before training with RL\n",
    "rl_net.generate_music_sequence(visualize_probs=True, title='pre_rl', length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_net.train(num_steps=1000000, exploration_period=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rewards received during training. Improves as chance of random exploration action decreases.\n",
    "rl_net.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards received during calls to evaluation function throughout training. \n",
    "# Does not include exploration or random actions.\n",
    "rl_net.plot_evaluation()"
   ]
  }
 ]
}